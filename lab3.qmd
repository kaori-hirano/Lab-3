---
title: "Lab 3: Cross Validation and Model Flexibility"
author: "Kaori Hirano"
date: "21/06/2023"
format: pdf
---

# Packages

```{r load-packages}
library(plyr)
suppressPackageStartupMessages(library(tidyverse))
library(boot)
suppressPackageStartupMessages(library(gbm))
```

# Data Simulation 

```{r simulate-data}
# set seed for replicability purposes
set.seed(280)
# the number of observations in our data set
n <- 1000
# x1 ~ beta(1.1, 1.1) * 20 - 10 (constrained to between -10 and 10)
x1 <- rbeta(n, shape1 = 1.1, shape2 = 1.1) * 20 - 10 


# Simulating n y1 values
y1 <- 2 + x1 - 0.012 * x1^2 + rnorm(n, mean = 0, sd = 1)
# And doing the same with y2
y2 <- 2 + sin(1.5 * sin(1.2 * x1) + 0.5 * cos(0.5 * (x1 - 1)^2)) + 0.3 * x1 +
  rnorm(n, mean = 0, sd = 1)


# putting all in a tibble
sim_data <- tibble(x1 = x1,
                   y1 = y1,
                   y2 = y2)
```

# Models

```{r mse-function}
# function takes two arguments:
## true - the true Y values
## pred - the predicted Y values
## observations in true and pred *must* be in same order
calc_mse <- function(true, pred, subset = NULL){
  error <- (true - pred)^2
  if(!is.null(subset)) error <- error[subset]
  mean(error)
}
# note that R functions don't need return statement - last object evaluated in
# function body gets returned
```

# Data Visualization

## Q1 

```{r q1}
y1_fun <- function(x1){
  2 + x1 - 0.012 * x1^2
}

ggplot(sim_data, aes(x1, y1)) + 
  geom_point(shape = 1, alpha = .5) + 
  labs(title = "Relationship between x1 and y1") + 
  geom_function(fun = y1_fun, linewidth = 1.25) + 
  theme_minimal() 

y2_fun <- function(x1){ 2 + sin(1.5 * sin(1.2 * x1) + 0.5 * cos(0.5 * (x1 - 1)^2)) + 0.3 * x1
  }

ggplot(sim_data, aes(x1, y2)) + 
  geom_point(shape = 1, alpha = .5) + 
  labs(title = "Relationship between x1 and y2") + 
  geom_function(fun = y2_fun, linewidth = 1.25) + 
  theme_minimal() #+
 # geom_line(linewidth = 1.25)

# fix line width problem, describe two relationships
```

# Validation Set Variance

## Q2 
```{r create-set}
set.seed(1) 
# take 294 samples without replacement from the vector 1:392
train <- sample(392, 294) # assigns 25% to validation, rest to test
```

## Q3
```{r predictions-glm}
my1 <- glm(y1 ~ x1, data = sim_data, subset = train)

my2 <- glm(y2 ~ x1, data = sim_data, subset = train)

y1_linear_mse <- calc_mse(sim_data$y1, predict(my1, sim_data), -train)

y2_linear_mse <- calc_mse(sim_data$y2, predict(my2, sim_data), -train)
```
y1 linear mse = 1.090
y2 linear mse = 1.569

```{r predictions-gbm}

# function to calculate predicted values
predict_gbm <- function(y_train, x_train, x_val){
  
  df_test <- data.frame(y = y_train,
                        x = x_train)
  
  boost_model <- gbm(y ~ x,
                     data = df_test,
                     interaction.depth = 1,
                     n.minobsinnode = as.integer(.05 * length(y_train)),
                     n.trees = 500,
                     shrinkage = .1,
                     bag.fraction = 1,
                     distribution = "gaussian")
  
  df_val <- data.frame(x = x_val)
  
  boost_pred <- predict(boost_model, newdata = df_val,
                        n.trees = 500)
  boost_pred
}

# fitting the two gbm models
y1_gbm_pred <- predict_gbm(sim_data$y1[train_index],
                           sim_data$x1[train_index], 
                           sim_data$x1[-train_index])

y2_gbm_pred <- predict_gbm(sim_data$y2[train_index],
                           sim_data$x1[train_index],
                           sim_data$x1[-train_index])

y1_gbm_mse <- calc_mse(sim_data$y1, predict(y1_gbm_pred, sim_data), -train)

y2_gbm_mse <- calc_mse(sim_data$y2, predict(y2_gbm_pred, sim_data), -train)
```

```{r comparing-mses}
cbind(y1_linear_mse, y1_gbm_mse, y2_linear_mse, y2_gbm_mse)
# something has gone wrong and this won't display, leading me t think there's an error in the code prior
#NOT DONE
```
## Q4

```{r q4}
# remember to remove eval: true when you get to this chunk while working
# through the lab!

# linear function
# insert code from Q1 here
# don't forget the +'s!
# drawing GBM predicted curve
  geom_function(aes(color = "GBM Model"), 
                    fun = predict_gbm, 
                args = list(x_train = sim_data$x1[train_index],
                            y_train = sim_data$y1[train_index]),
                linewidth = 1.25)

# nonlinear function
# insert code from Q1 here
# don't forget the +'s!
# drawing GBM predicted curve
  geom_function(aes(color = "GBM Model"), 
                    fun = predict_gbm, 
                args = list(x_train = sim_data$x1[train_index],
                            y_train = sim_data$y2[train_index]),
                linewidth = 1.25)
```

## Q5
```{r chunk-name-here}
# code here
```

# K-Fold Cross Validation

## Q6
```{r chunk-name-here}
# code here
```
The GBM is better than the linear model for nonlinear data. 

## Q7

```{r cv-gbm}
#| eval: true
#| warning: false
# remember to remove eval: true when you get to this chunk while working
# through the lab!

# gbm has bug when cross-validating with a single predictor
# so we are adding a non-informative predictor
# gbm throws a warning, but it's their bug! 
sim_data$x2 <- 0

set.seed(4)
y1_gbm_cv <- gbm(y1 ~ x1 + x2,
                     data = sim_data,
                     interaction.depth = 1,
                     n.minobsinnode = 5,
                     n.trees = 500,
                     shrinkage = .1,
                     bag.fraction = 1,
                     cv.folds = 10,
                     distribution = "gaussian")$cv.error[500]

y1_gbm_cv
```
a) 

b) In this case, we don't care a lot about interpretability considering we haven't even named our variables beyond x and y. Because we aren't trying to make interpretations from the data and are just concerned about finding the best fit, favoring flexibility even though it takes longer is preferred in this case. 


